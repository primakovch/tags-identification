{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import unicodedata2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "STOP=['and','have','been','is','are','a','an','the','or','has','many']\n",
    "def basic_clean(text):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')+STOP\n",
    "    text = (unicodedata2.normalize('NFKD', text)\n",
    "            .encode('ascii', 'ignore')\n",
    "            .decode('utf-8', 'ignore')\n",
    "            .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    #return words\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "\n",
    "def get_best_thresholds(true, preds):\n",
    "    thresholds = [i/100 for i in range(100)]\n",
    "    best_thresholds = []\n",
    "    for idx in range(25):\n",
    "        f1_scores = [f1_score(true[:, idx], (preds[:, idx] > thresh) * 1)\n",
    "                     for thresh in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "        best_thresholds.append(best_thresh)\n",
    "\n",
    "    return best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['Analysis of PDEs', 'Applications',\n",
    "              'Artificial Intelligence', 'Astrophysics of Galaxies',\n",
    "              'Computation and Language', 'Computer Vision and Pattern Recognition',\n",
    "              'Cosmology and Nongalactic Astrophysics',\n",
    "              'Data Structures and Algorithms', 'Differential Geometry',\n",
    "              'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n",
    "              'Information Theory', 'Instrumentation and Methods for Astrophysics',\n",
    "              'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n",
    "              'Optimization and Control', 'Representation Theory', 'Robotics',\n",
    "              'Social and Information Networks', 'Statistics Theory',\n",
    "              'Strongly Correlated Electrons', 'Superconductivity',\n",
    "              'Systems and Control']\n",
    "\n",
    "\n",
    "topic_col = ['Computer Science', 'Mathematics', 'Physics', 'Statistics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, val = train_test_split(train, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tr = basic_clean(''.join(str(train['ABSTRACT'].tolist())))\n",
    "words_tst = basic_clean(''.join(str(test['ABSTRACT'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['ABSTRACT']=val['ABSTRACT'].map(lambda txt:basic_clean(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10530    [prove, existence, hermitianeinstein, metric, ...\n",
       "3174     [novel, predictor, considering, traffic, flow,...\n",
       "3240     [inside, paper, introduce, kumaraswamy, autore...\n",
       "8368     [recent, financial, upheaval, cast, doubt, ade...\n",
       "11359    [analyze, generalized, dirac, system, dispersi...\n",
       "                               ...                        \n",
       "2471     [surge, inside, availability, genomic, data, h...\n",
       "7509     [new, type, endtoend, system, considering, tex...\n",
       "3090     [central, question, inside, science, science, ...\n",
       "6145     [paper, study, empirical, risk, minimization, ...\n",
       "7614     [oneopposition, nearearth, asteroid, neas, gro...\n",
       "Name: ABSTRACT, Length: 2801, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['ABSTRACT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twog(txt):\n",
    "    words_ts=pd.Series(nltk.ngrams(txt, 2))\n",
    "    words_tst=[]\n",
    "    for i in range(len(words_ts)):\n",
    "        words_tst.append(words_ts[i][0])\n",
    "        \n",
    "    #words_tst.append(words_ts[len(words_ts)][1])\n",
    "\n",
    "    return words_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=twog(val['ABSTRACT'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['ABSTRACT']=val['ABSTRACT'].map(lambda txt:twog(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10530    [prove, existence, hermitianeinstein, metric, ...\n",
       "3174     [novel, predictor, considering, traffic, flow,...\n",
       "3240     [inside, paper, introduce, kumaraswamy, autore...\n",
       "8368     [recent, financial, upheaval, cast, doubt, ade...\n",
       "11359    [analyze, generalized, dirac, system, dispersi...\n",
       "Name: ABSTRACT, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn['ABSTRACT']=trn['ABSTRACT'].map(lambda txt:basic_clean(txt))\n",
    "trn['ABSTRACT']=trn['ABSTRACT'].map(lambda txt:twog(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec3 = TfidfVectorizer(max_features=10000)\n",
    "_ = vec3.fit(list(train['ABSTRACT']))\n",
    "\n",
    "trn_abs = vec3.transform(trn['ABSTRACT'])\n",
    "val_abs = vec3.transform(val['ABSTRACT'])\n",
    "#tst_abs = vec3.transform(test['ABSTRACT'])\n",
    "\n",
    "trn3 = np.hstack((trn_abs.toarray(), trn[topic_col]))\n",
    "val3 = np.hstack((val_abs.toarray(), val[topic_col]))\n",
    "#tst3 = np.hstack((tst_abs.toarray(), test[topic_col]))\n",
    "\n",
    "trn3 = csr_matrix(trn3.astype('int16'))\n",
    "val3 = csr_matrix(val3.astype('int16'))\n",
    "#tst3 = csr_matrix(tst3.astype('int16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
