{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import unicodedata2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "STOP=['and','have','been','is','are','a','an','the','or','has','many']\n",
    "def basic_clean(text):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')+STOP\n",
    "    text = (unicodedata2.normalize('NFKD', text)\n",
    "            .encode('ascii', 'ignore')\n",
    "            .decode('utf-8', 'ignore')\n",
    "            .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    #return words\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "\n",
    "def get_best_thresholds(true, preds):\n",
    "    thresholds = [i/100 for i in range(100)]\n",
    "    best_thresholds = []\n",
    "    for idx in range(25):\n",
    "        f1_scores = [f1_score(true[:, idx], (preds[:, idx] > thresh) * 1)\n",
    "                     for thresh in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "        best_thresholds.append(best_thresh)\n",
    "\n",
    "    return best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    text = ' '.join(text.split())  \n",
    "    text = text.lower() \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ABSTRACT']=train['ABSTRACT'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        a ever growing datasets inside observational a...\n",
       "1        we propose the framework considering optimal t...\n",
       "2        nanostructures with open shell transition meta...\n",
       "3        stars are self gravitating fluids inside which...\n",
       "4        deep neural perception and control networks ar...\n",
       "                               ...                        \n",
       "13999    a methodology of automatic detection of a even...\n",
       "14000    we consider a case inside which the robot has ...\n",
       "14001    despite being usually considered two competing...\n",
       "14002    we present the framework and its implementatio...\n",
       "14003    here we report small angle neutron scattering ...\n",
       "Name: ABSTRACT, Length: 14004, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['ABSTRACT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =  nltk.corpus.stopwords.words('english')+['n','k','x','c','r','h','g','p','inside','considering']\n",
    "def remove_stopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ABSTRACT']=train['ABSTRACT'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ever growing datasets observational astronomy ...\n",
       "1        propose framework optimal matchings excluding ...\n",
       "2        nanostructures open shell transition metal mol...\n",
       "3        stars self gravitating fluids pressure buoyanc...\n",
       "4        deep neural perception control networks likely...\n",
       "                               ...                        \n",
       "13999    methodology automatic detection event basis in...\n",
       "14000    consider case robot navigate unknown environme...\n",
       "14001    despite usually considered two competing pheno...\n",
       "14002    present framework implementation relying natur...\n",
       "14003    report small angle neutron scattering sans mea...\n",
       "Name: ABSTRACT, Length: 14004, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['ABSTRACT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(x, terms = 30): \n",
    "  all_words = ' '.join([text for text in x]) \n",
    "  all_words = all_words.split() \n",
    "  fdist = nltk.FreqDist(all_words) \n",
    "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) \n",
    "  \n",
    "  # selecting top 20 most frequent words \n",
    "  d = words_df.nlargest(columns=\"count\", n = terms) \n",
    "  \n",
    "  # visualize words and frequencies\n",
    "  plt.figure(figsize=(12,15)) \n",
    "  ax = sns.barplot(data=d, x= \"count\", y = \"word\") \n",
    "  ax.set(ylabel = 'Word') \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ABSTRACT']=test['ABSTRACT'].apply(lambda x: clean_text(x))\n",
    "test['ABSTRACT']=test['ABSTRACT'].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=40000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(max_features=40000)\n",
    "combined = list(train['ABSTRACT']) + list(test['ABSTRACT'])\n",
    "vec.fit(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, val = train_test_split(train, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_COLS=['Computer Science', 'Mathematics', 'Physics', 'Statistics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_abs = vec.transform(trn['ABSTRACT'])\n",
    "val_abs = vec.transform(val['ABSTRACT'])\n",
    "tst_abs = vec.transform(test['ABSTRACT'])\n",
    "\n",
    "trn2 = np.hstack((trn_abs.toarray(), trn[TOPIC_COLS]))\n",
    "val2 = np.hstack((val_abs.toarray(), val[TOPIC_COLS]))\n",
    "tst2 = np.hstack((tst_abs.toarray(), test[TOPIC_COLS]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11203, 40004) (2801, 40004) (6002, 40004)\n"
     ]
    }
   ],
   "source": [
    "print(trn2.shape, val2.shape, tst2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn2 = csr_matrix(trn2.astype('int16'))\n",
    "val2 = csr_matrix(val2.astype('int16'))\n",
    "tst2 = csr_matrix(tst2.astype('int16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int16')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLS=['Analysis of PDEs', 'Applications',\n",
    "       'Artificial Intelligence', 'Astrophysics of Galaxies',\n",
    "       'Computation and Language', 'Computer Vision and Pattern Recognition',\n",
    "       'Cosmology and Nongalactic Astrophysics',\n",
    "       'Data Structures and Algorithms', 'Differential Geometry',\n",
    "       'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n",
    "       'Information Theory', 'Instrumentation and Methods for Astrophysics',\n",
    "       'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n",
    "       'Optimization and Control', 'Representation Theory', 'Robotics',\n",
    "       'Social and Information Networks', 'Statistics Theory',\n",
    "       'Strongly Correlated Electrons', 'Superconductivity',\n",
    "       'Systems and Control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = OneVsRestClassifier(LogisticRegression(C = 2, n_jobs=-1))\n",
    "_  = clf.fit(trn2, trn[TARGET_COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col=TARGET_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = clf.predict_proba(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds = get_best_thresholds(val[target_col].values, val_preds)\n",
    "\n",
    "for i, thresh in enumerate(best_thresholds):\n",
    "    val_preds[:, i] = (val_preds[:, i] > thresh) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(target,predict):\n",
    "    print(\"f1:\", f1_score(target, predict, average='micro'))\n",
    "    print(\"Precision:\", precision_score(target, predict, average='micro'))\n",
    "    print(\"Recall:\",recall_score(target, predict, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.7269837388433794\n",
      "Precision: 0.6714092140921409\n",
      "Recall: 0.7925886430285257\n"
     ]
    }
   ],
   "source": [
    "score(val[target_col], val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = clf.predict_proba(tst2)\n",
    "\n",
    "for i, thresh in enumerate(best_thresholds):\n",
    "  preds_test[:, i] = (preds_test[:, i] > thresh) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test['id']\n",
    "submission[target_col] = preds_test\n",
    "submission.to_csv('sol7.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = ' '\n",
    "test['text'] = ' '\n",
    "\n",
    "\n",
    "train['text'] += train['ABSTRACT']\n",
    "test['text'] += test['ABSTRACT']\n",
    "\n",
    "trn, val = train_test_split(train, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tok = Tokenizer(num_words = 1000000)\n",
    "tok.fit_on_texts(train['text'].str.lower().tolist() + test['text'].str.lower().tolist())\n",
    "\n",
    "vocab_size = len(tok.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = tok.texts_to_sequences(trn['text'])\n",
    "X_val = tok.texts_to_sequences(val['text'])\n",
    "X_test = tok.texts_to_sequences(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_trn = pad_sequences(X_trn, maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 50)           2259550   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 198, 64)           9664      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 196, 100)          19300     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 194, 100)          30100     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 192, 48)           14448     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 25)                230425    \n",
      "=================================================================\n",
      "Total params: 2,563,487\n",
      "Trainable params: 2,563,487\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, SpatialDropout1D, LSTM\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Convolution1D\n",
    "\n",
    "\n",
    "embedding_dim = 50\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "model.add(Conv1D(64, 3, activation='sigmoid'))\n",
    "model.add(Conv1D(100, 3, activation='sigmoid'))\n",
    "model.add(Conv1D(100, 3, activation='sigmoid'))\n",
    "# model.add(Dropout(0.70))\n",
    "model.add(Conv1D(48, 3, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25, activation='sigmoid', name = 'Output'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "44/44 [==============================] - 2s 51ms/step - loss: 0.2439 - accuracy: 0.1570 - val_loss: 0.1962 - val_accuracy: 0.1649\n",
      "Epoch 2/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1961 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 3/32\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1961 - val_accuracy: 0.1649\n",
      "Epoch 4/32\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1960 - val_accuracy: 0.1649\n",
      "Epoch 5/32\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1959 - val_accuracy: 0.1649\n",
      "Epoch 6/32\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1960 - val_accuracy: 0.1649\n",
      "Epoch 7/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1961 - val_accuracy: 0.1649\n",
      "Epoch 8/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1961 - accuracy: 0.1729 - val_loss: 0.1963 - val_accuracy: 0.1649\n",
      "Epoch 9/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1961 - val_accuracy: 0.1649\n",
      "Epoch 10/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1959 - val_accuracy: 0.1649\n",
      "Epoch 11/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1959 - val_accuracy: 0.1649\n",
      "Epoch 12/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1960 - accuracy: 0.1729 - val_loss: 0.1961 - val_accuracy: 0.1649\n",
      "Epoch 13/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1959 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 14/32\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 15/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 16/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 17/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 18/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 19/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1958 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 20/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 21/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 22/32\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 23/32\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 24/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 25/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 26/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 27/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 28/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 29/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 30/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 31/32\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n",
      "Epoch 32/32\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1957 - accuracy: 0.1729 - val_loss: 0.1958 - val_accuracy: 0.1649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b05c891cf8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_trn, trn[TARGET_COLS], validation_data=(X_val, val[TARGET_COLS]), verbose=True, epochs=32, batch_size=256,\n",
    "          callbacks = [tf.keras.callbacks.ReduceLROnPlateau()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10168618520928216"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds1 = model.predict(X_val)\n",
    "best_thresholds = get_best_thresholds(val[TARGET_COLS].values, val_preds1)\n",
    "for i, thresh in enumerate(best_thresholds):\n",
    "  val_preds1[:, i] = (val_preds[:, i] > thresh) * 1\n",
    "f1_score(val[TARGET_COLS], val_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.10168618520928216\n",
      "Precision: 0.05356658336308461\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "score(val[target_col], val_preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=(10*val_preds+val_preds1)/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, thresh in enumerate(best_thresholds):\n",
    "  pred[:, i] = (pred[:, i] > thresh) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.34179559816953586\n",
      "Precision: 0.21478945566586785\n",
      "Recall: 0.8363103172487337\n"
     ]
    }
   ],
   "source": [
    "score(val[target_col], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
